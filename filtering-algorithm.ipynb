{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Config Data and Define Processing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "PROCESSING_INFO = config_data['PROCESSING_INFO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_identifier(length=30):\n",
    "    \"\"\"Generate a random identifier of a given length.\"\"\"\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    identifier = ''.join(random.choice(characters) for _ in range(length))\n",
    "    return identifier\n",
    "\n",
    "def preprocess_df(df, processing_info):\n",
    "    \"\"\"Preprocesses the input DataFrame by setting correct dtypes, filling NaN, filtering unnecessary fields.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame to be preprocessed.\n",
    "        processing_info (dict): Information about the processing setup.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    data = df[processing_info[\"raw_columns\"]].copy()\n",
    "    mapping = dict(zip(processing_info[\"raw_columns\"], processing_info[\"new_columns\"]))\n",
    "    data.columns = [mapping[col] for col in data.columns]\n",
    "\n",
    "    data[\"day\"] = pd.to_datetime(data[\"day\"])\n",
    "    # data[\"ad_set\"] = data[\"ad_set\"].astype(str)\n",
    "\n",
    "    identifiers = [generate_identifier() for _ in range(data[\"ad_set\"].nunique())]\n",
    "    assert len(set(identifiers)) == data[\"ad_set\"].nunique()\n",
    "    mapping = dict(zip(data[\"ad_set\"].unique(), list(identifiers)))\n",
    "\n",
    "    data[\"ad_set\"] = data[\"ad_set\"].apply(lambda x: mapping[x])\n",
    "    \n",
    "    data[\"clicks\"] = data[\"clicks\"].astype(float)\n",
    "    data[\"impressions\"] = data[\"impressions\"].astype(float)\n",
    "\n",
    "    if not data[\"time_of_day\"].dtype == int:\n",
    "        data[\"hour\"] = pd.to_datetime(data[\"time_of_day\"].apply(lambda x: x.split(\" - \")[0])).dt.hour + 1\n",
    "    else:\n",
    "        data[\"hour\"] = data[\"time_of_day\"]\n",
    "    data = data[processing_info[\"init_fields\"]]\n",
    "\n",
    "    temp = pd.DataFrame({col: [] for col in data.columns})\n",
    "    temp[\"day_num\"] = []\n",
    "\n",
    "    for add in data[\"ad_set\"].unique():\n",
    "       cons_days = data[data[\"ad_set\"] == add][\"day\"].unique()[:processing_info[\"days\"]]\n",
    "       days_map = {day: i+1 for i, day in enumerate(cons_days)}\n",
    "       df = data[(data[\"ad_set\"] == add) & (data[\"day\"].isin(cons_days))].copy()\n",
    "       df[\"day_num\"] = df[\"day\"].apply(lambda x: days_map[x])\n",
    "       temp = pd.concat([temp, df])\n",
    "    \n",
    "    data = temp\n",
    "    data = data.sort_values([\"ad_set\", \"day_num\", \"hour\"]).reset_index(drop=True)\n",
    "\n",
    "    data[\"clicks\"].fillna(0, inplace=True)\n",
    "    data[\"spend\"].fillna(0, inplace=True)\n",
    "    data[\"purchases\"].fillna(0, inplace=True)\n",
    "    data[\"leads\"].fillna(0, inplace=True)\n",
    "    data[\"impressions\"].fillna(0, inplace=True)\n",
    "    data[\"revenue\"] = data[\"purchases\"]*processing_info[\"ltv\"]\n",
    "\n",
    "    data[\"cpl\"] = data[\"spend\"] / data[\"leads\"]\n",
    "    data[\"ctr\"] = data[\"clicks\"] / data[\"impressions\"]\n",
    "    data[\"cpm\"] = data[\"spend\"] / data[\"impressions\"] * 1000\n",
    "\n",
    "    res = data.groupby(\"ad_set\")[\"spend\"].count()\n",
    "    ad_sets = res[res >= 24*processing_info[\"days\"]].index\n",
    "    data = data[data[\"ad_set\"].isin(ad_sets)]\n",
    "\n",
    "    return data\n",
    "\n",
    "def label_sucess(df):\n",
    "    \"\"\"Labels success based on total ROAS.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame with revenue and spend columns.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with success column added.\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "\n",
    "    spend = data.groupby(\"ad_set\")[\"spend\"].sum()\n",
    "    revenue = data.groupby(\"ad_set\")[\"revenue\"].sum()\n",
    "\n",
    "    roas = revenue / spend\n",
    "    success = {add: 1 if roas.loc[add] > 1 else 0 for add in roas.index}\n",
    "    data[\"success\"] = data[\"ad_set\"].apply(lambda x: success[x])\n",
    "\n",
    "    return data\n",
    "\n",
    "def calculate_cummulatives(df, processing_info):\n",
    "    \"\"\"Calculates cumulative metrics for specified fields.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame with necessary columns.\n",
    "        processing_info (dict): Information about processing setup.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with cumulative metrics calculated.\n",
    "    \"\"\"\n",
    "    temp = pd.DataFrame(columns=df.columns)\n",
    "    for id in df[\"ad_set\"].unique():\n",
    "        subset = df[df[\"ad_set\"] == id].copy().sort_values([\"ad_set\", \"day_num\", \"hour\"]).reset_index(drop=True)\n",
    "        for metric in processing_info[\"cumm_fields\"]:\n",
    "            subset[f\"{metric}_cumm\"] = np.where(~np.isfinite(subset[f\"{metric}\"]), 0, subset[f\"{metric}\"]).cumsum()\n",
    "        \n",
    "        subset[\"cpl_sub\"] = subset[\"spend_cumm\"] / subset[\"leads_cumm\"]\n",
    "        subset[\"ctr_sub\"] = subset[\"clicks_cumm\"] / subset[\"impressions_cumm\"]\n",
    "        subset[\"cpm_sub\"] = subset[\"spend_cumm\"] / subset[\"impressions_cumm\"] * 1000\n",
    "        subset[\"roas_sub\"] = subset[\"revenue_cumm\"] / subset[\"spend_cumm\"]\n",
    "\n",
    "        subset[\"cpl_sub\"] = np.where(~(np.isfinite(subset[\"cpl_sub\"])), 0, subset[\"cpl_sub\"])\n",
    "        subset[\"ctr_sub\"] = np.where(~(np.isfinite(subset[\"ctr_sub\"])), 0, subset[\"ctr_sub\"])\n",
    "        subset[\"cpm_sub\"] = np.where(~(np.isfinite(subset[\"cpm_sub\"])), 0, subset[\"cpm_sub\"])\n",
    "        subset[\"roas_sub\"] = np.where(~(np.isfinite(subset[\"roas_sub\"])), 0, subset[\"roas_sub\"])\n",
    "\n",
    "        temp = pd.concat([temp, subset])\n",
    "\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "    return temp\n",
    "\n",
    "def calculate_increases(df, processing_info):\n",
    "    \"\"\"Calculates percentage increases for specified fields.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame with necessary columns.\n",
    "        processing_info (dict): Information about processing setup.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with percentage increases calculated.\n",
    "    \"\"\"\n",
    "    temp = pd.DataFrame(columns=df.columns)\n",
    "    for id in df[\"ad_set\"].unique():\n",
    "        subset = df[df[\"ad_set\"] == id].copy().sort_values([\"ad_set\", \"day\", \"hour\"]).reset_index(drop=True)\n",
    "        for metric in processing_info[\"incr_fields\"]:\n",
    "            subset[f\"{metric}_incr\"] = ((subset[f\"{metric}\"] - subset[f\"{metric}\"].shift(1)) / subset[f\"{metric}\"].shift(1) * 100).fillna(0)\n",
    "            subset[f\"{metric}_incr\"] = np.where(~np.isfinite(subset[f\"{metric}_incr\"]), 0, subset[f\"{metric}_incr\"])\n",
    "        temp = pd.concat([temp, subset])\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "    return temp\n",
    "\n",
    "def process_df(filename, processing_info=PROCESSING_INFO, prefix = \"\", to_return=False):\n",
    "    \"\"\"Prepares the DataFrame for model training and Thompson Sampling evaluation.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the file to be processed.\n",
    "        processing_info (dict, optional): Information about processing setup. Defaults to PROCESSING_INFO.\n",
    "        prefix (str, optional): Prefix to be added to the processed filename. Defaults to \"\".\n",
    "        to_return (bool, optional): Whether to return the processed DataFrame. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame or None: Processed DataFrame if to_return is True, else None.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(processing_info[\"raw_folder\"] + filename)\n",
    "    df = preprocess_df(df, processing_info)\n",
    "    df = label_sucess(df)\n",
    "    df = calculate_cummulatives(df, processing_info)\n",
    "    df = calculate_increases(df, processing_info)\n",
    "\n",
    "    df.to_csv(processing_info[\"processed_folder\"] + prefix + filename, index=False)\n",
    "\n",
    "    if to_return:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:41<00:00,  8.26s/it]\n"
     ]
    }
   ],
   "source": [
    "FILES = [\n",
    "    \"01-14.04.2023.csv\",\n",
    "    \"03-09.09.2023.csv\",\n",
    "    \"01-07.11.2023.csv\",\n",
    "    \"12-18.12.2023.csv\",\n",
    "    \"15-17.02.2024.csv\",\n",
    "]\n",
    "\n",
    "for filename in tqdm(FILES):\n",
    "    data = process_df(filename, prefix=\"\", to_return=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
